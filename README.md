# Crawling-Chimp ğŸµğŸ•·ï¸
**Enhanced Recursive Web Crawler with Authentication**

Imagine this script as a troop of adventurous chimps exploring the vast jungle of the internet. Each chimp represents a link found on a webpage. They swing from one link to another, shouting with excitement as they discover new paths. Some chimps are slower, taking their time to savor the journey, while others are swift and curious. At the end of their exploration, they compile a list of all the intriguing spots they've visited, much like our script saves the output links to a file. It's a jungle out there, but with this script, your chimpsâ€”I mean, linksâ€”are in good hands!

## âœ¨ New Features (v2.0)

- **ğŸ” Form-based Authentication**: Automatically detects and logs into websites
- **ğŸ•·ï¸ Enhanced Recursive Crawling**: Improved link discovery algorithms
- **ğŸ“Š Comprehensive Logging**: Multiple log levels with detailed progress tracking
- **ğŸ›¡ï¸ Respectful Crawling**: Follows robots.txt and includes smart delays
- **âš™ï¸ Highly Configurable**: Depth limits, page limits, custom headers
- **ğŸ’¾ Session Management**: Maintains login state throughout crawling
- **ğŸ“ Clean Output**: Organized results with timestamps and statistics

## ğŸš€ Quick Start

### Basic Usage
```bash
python Crawling_Chimp.py -u https://example.com --username user@email.com --password yourpassword
```

### Advanced Usage
```bash
python Crawling_Chimp.py -u https://example.com --username user@email.com --password yourpassword -f results.txt --log-level INFO -d 3 -p 200
```

## ğŸ“‹ Installation

1. Clone the repository:
```bash
git clone https://github.com/danahkh/Crawling-Chimp.git
cd Crawling-Chimp
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“– Usage Options

```bash
usage: Crawling_Chimp.py [-h] -u URL [-s] [-f OUTPUT_FILE] [-d MAX_DEPTH] [-p MAX_PAGES] 
                         [--log-level {DEBUG,INFO,WARNING,ERROR}] [--log-file LOG_FILE] 
                         [--cred-file CRED_FILE] [--username USERNAME] [--password PASSWORD] 
                         [--create-cred-template] [--save-session SAVE_SESSION] 
                         [--load-session LOAD_SESSION] [--headers HEADERS]

Crawling Chimp - Enhanced Recursive Web Crawler

options:
  -h, --help            show this help message and exit
  -u URL, --url URL     URL to scrape
  -s, --slow            Slow down requests (1 second delay)
  -f OUTPUT_FILE, --output-file OUTPUT_FILE
                        Output file to save the discovered links
  -d MAX_DEPTH, --max-depth MAX_DEPTH
                        Maximum crawling depth (default: 3)
  -p MAX_PAGES, --max-pages MAX_PAGES
                        Maximum pages to crawl (default: 100)
  --log-level {DEBUG,INFO,WARNING,ERROR}
                        Logging level (default: INFO)
  --log-file LOG_FILE   Log file path (default: console only)
  --cred-file CRED_FILE
                        JSON file containing credentials
  --username USERNAME   Username for basic authentication
  --password PASSWORD   Password for basic authentication
  --create-cred-template
                        Create a sample credentials file template
  --save-session SAVE_SESSION
                        Save session cookies to file
  --load-session LOAD_SESSION
                        Load session cookies from file
  --headers HEADERS     Custom headers as JSON string
```

## ğŸ” Authentication

### Using Command Line Credentials
```bash
python Crawling_Chimp.py -u https://example.com --username user@email.com --password yourpassword
```

### Using Credentials File (Recommended)
1. Create a credentials template:
```bash
python Crawling_Chimp.py --create-cred-template
```

2. Edit the `credentials.json` file with your actual credentials

3. Use the credentials file:
```bash
python Crawling_Chimp.py -u https://example.com --cred-file credentials.json
```

## ğŸ“Š Example Output

```
============================================================
CRAWLING SUMMARY
============================================================
Starting URL: https://example.com
Pages crawled: 100
Unique links found: 123
Max depth: 3
Duration: 0:00:34.441287
============================================================
```

## ğŸ›¡ï¸ Security Features

- **Robots.txt Compliance**: Checks and respects robots.txt files
- **Rate Limiting**: Built-in delays to prevent overwhelming servers
- **Credential Protection**: Secure handling of authentication data
- **Session Management**: Proper cleanup of authentication sessions

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

[MIT](https://choosealicense.com/licenses/mit/)
